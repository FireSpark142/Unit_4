{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Wrangling with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenStreetMap is an open source alternative to Google Maps.\n",
    "\n",
    "http://www.openstreetmap.org\n",
    "\n",
    "Users can map things such as nodes, streets, spots of interest, polylines of buildings, etc. The data is stored in XML, for ease of analysis although it is a propriety version of XML called OSM XML.\n",
    "http://wiki.openstreetmap.org/wiki/OSM_XML\n",
    "\n",
    "Some highlights of the OSM XML format relevent to this project are:\n",
    "\n",
    "* OSM XML is list of instances of data primatives (nodes, ways, and relations) within a given location.\n",
    "* Nodes are abstract representations of physical locations.\n",
    "* Ways are representative of pathways, for human, and non-human pathing. \n",
    "* Nodes and ways both have children tag elements of key value pairs of descriptive information about the parent node or way.\n",
    "* Due to it being user generated and open source, there is likely going to be dirty data. This project's goals are auditing, cleaning, and data summarization using Python 2.7 and SQLite.\n",
    "\n",
    "#### Location: St.Charles, MO, USA\n",
    "Data Sources:\n",
    "\n",
    "https://overpass-api.de/api/map?bbox=-90.7189,38.7013,-90.3117,38.8884\n",
    "\n",
    "The unzipped data was 176 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files included:\n",
    "* OpenStreetMap Data Wrangling with SQL.ipynb - the jupyter notebook you're currently reading.\n",
    "* Audit.py - includes the update functions, as well as the intial audit used to create the update functions.\n",
    "* OSM_to_CSV.py - iterates through the OSM file, calls the update functions from the audit.py file and then seperates the values into\n",
    "their appropriate csv file. The csv file is then checked against the schema.py for proper database schema.\n",
    "* schema.py - this is a file that is the python equivelant of the database_wrangling_schema.sql that is used to verify the data is formatted properly for database upload.\n",
    "* data_wrangling_schema.sql - the file that schema.py is based off, is not used but is included for reference.\n",
    "* creating_db.py - this file creates the database and the tables inside. As well as checking for duplicate tables and then inserts the data from the converted .csv files \n",
    "into their proper table.\n",
    "* queries.py - this file contains the queries used for our data exploration phase.\n",
    "* sample1percent.osm - a sample of the dataset that is 1% of the size or every 100 top level lines.\n",
    "* nodes.csv, nodes_tags.csv, ways.csv, ways_nodes.csv, ways_tags.csv - the csv files created from the OSM_to_CSV.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered:\n",
    "After taking a small sample of the dataset using Sample_Streetmap.py, I used Audit.py to audit the sample data. I noticed following problems:\n",
    "\n",
    "* Abbreviated street types (‘St’, ‘ST’, ‘Pkwy’, ‘Ctr’, etc).\n",
    "* Abbreviated street directional indicators, such as N for North and S for South, like N. Broadway, instead of North Broadway.\n",
    "* Inconsistent street types and directional indicators, if all types were abbreviated, but since they weren't I decided to correct them to their non abbreivated forms.\n",
    "* Fixing the abbreivated street directional indicators caused an issue when other streets such as Route N would be changed to Route North. This was incorrect and so we had to find a way to fix the directional indicators while leaving the Routes and Highways with N, S, E, or W alone.\n",
    "* Surprisingly, there was only a single postal code issue. Unfortunately this meant we still needed to correct all of them.\n",
    "* City names were often plain wrong, in the case of one being called \"drive-through\" and we also had the issue of many different versions of the Saint prefix which is very common in this dataset. So we had to fix the incorrect ones and standardize any inconsistencies.\n",
    "* Time, auditing 176MB OSM file takes a VERY long time when done iteratively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Makes a dictionary of all of the street types to allow us to create a list to update\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "#If the element key for 'k' is 'addr:street', return the associated value pair\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "\"\"\" Here we create a dictionary of type set called street_types, \n",
    "and turn the open function into a variable for ease of use in the future \n",
    "Next is my pride and joy, instead of using \"for et.iterparse\" to iterate directly line by line\n",
    "through the file instead we use the osm_file var to open the file in memory, and \n",
    "then turn it into an iterable. This saves a TON of time, as we can iterate on the file\n",
    "in memory instead of iterating the file line by line. Once we do this, we then iterate through and\n",
    "for each tag that matches \"node\" or \"way\", we check if it is a street name, and if so we run the audit_street_types function.\n",
    "we then clear the root tree, saving memory and time, close the file, and return the updated street_types dict.\n",
    "\"\"\"\n",
    "\n",
    "def audit_s(osmfile):\n",
    "    street_types = defaultdict(set)\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "\n",
    "    # get an iterable\n",
    "    iterable = ET.iterparse(osm_file, events=(\"start\", \"end\"))\n",
    "\n",
    "    # turn it into an iterator\n",
    "    iterable = iter(iterable)\n",
    "\n",
    "    # get the root element\n",
    "    event, root = iterable.next()\n",
    "\n",
    "    for event, elem in iterable:\n",
    "        if event == \"end\" and (elem.tag == \"node\" or elem.tag == \"way\"):\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "        root.clear()\n",
    "    \n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\"\"\" The update_street function takes the information we learned from the audit_s function\n",
    "and utilizes that to check a manually created mapping dictionary and DONT_UPDATE tuple.\n",
    "These two objects are created by reading the report from audit_s and choosing how we want to standardize the types.\n",
    "to go above and beyond, we also standardized prefixes such as N for North. Unfortunely this caused an issue where Highway or Route, \n",
    "which often had the suffix N would be incorrectly corrected to North, such as Route North.\n",
    "Therefore we created the DON_UPDATE tuple, and check each value against the tuple, and if there is a match\n",
    "the value is not updated. To fix the street types, we broke the value into parts\n",
    "seperated by whitespace using .split(), then change the value if it matches the key found in mapping, to the paired value.\n",
    "Finally, the seperated parts are then rejoined with a space inbetween using the .join() function.\n",
    "\"\"\"\n",
    "def update_street(name):\n",
    "    mapping = {\"St\": \"Street\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Rd\": \"Road\",\n",
    "           \"N.\": \"North\",\n",
    "           \"N\": \"North\",\n",
    "           \"S.\": \"South\",\n",
    "           \"Blvd\": \"Boulevard\",\n",
    "           \"Blvd.\": \"Boulevard\",\n",
    "           \"Expy\": \"Expressway\",\n",
    "           \"Ln\": \"Lane\",\n",
    "           \"Ctr\": \"Center\",\n",
    "           \"Ctr.\": \"Center\",\n",
    "           \"5th\": \"Fifth\",\n",
    "           \"4th\": \"Fourth\",\n",
    "           \"3rd\": \"Third\",\n",
    "           \"2nd\": \"Second\",\n",
    "           \"1st\": \"First\",\n",
    "           #There was a street named just dade...that's it..so I went on google to find the real address, so this corrects that occurance.\n",
    "           \"Dade\": \"South Dade Avenue\",\n",
    "           \"MO-94\": \"Highway 94\"\n",
    "          }\n",
    "\n",
    "    DONT_UPDATE = ('route','suite')\n",
    "\n",
    "    if name.lower().startswith(DONT_UPDATE):\n",
    "        return name\n",
    "    else: \n",
    "        return ' '.join(mapping.get(part, part).title() for part in name.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrect Postal Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a single postal code which was obviously a user error, which the user accidentally added an extra digit to the end this was fixed in the audit.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicti(data, item):\n",
    "    \"\"\"This function creates a dictionary where postcodes can be held.\n",
    "    The dictionary key will be the postcode itself and the dictionary value\n",
    "    is a count of postcodes that were repeated throughout the dataset.\"\"\"\n",
    "    data[item] += 1\n",
    "\n",
    "#This function returns the elem if 'k' matches \"addr:postcode\"\n",
    "def is_postcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "#This codes is identical in function the the street function of similar name\n",
    "def audit_p(osmfile):\n",
    "    osm_file = open(OSMFILE, \"r\")\n",
    "    data = defaultdict(int)\n",
    "\n",
    "    # get an iterable\n",
    "    iterable = ET.iterparse(osm_file, events=(\"start\", \"end\"))\n",
    "\n",
    "    # turn it into an iterator\n",
    "    iterable = iter(iterable)\n",
    "\n",
    "    # get the root element\n",
    "    event, root = iterable.next()\n",
    "\n",
    "    for event, elem in iterable:\n",
    "        if event == \"end\" and (elem.tag == \"node\" or elem.tag == \"way\"):\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postcode(tag):\n",
    "                    dicti(data, tag.attrib['v'])\n",
    "        root.clear()\n",
    "    osm_file.close()\n",
    "    return data\n",
    "\n",
    "# This is the function that actually changes the post code to the proper values\n",
    "# It is called in the OSM_to_XML file, when writing the changes to the .csv\n",
    "\n",
    "def update_postcode(postcodes):\n",
    "    output = list()\n",
    "    \n",
    "    if re.search(postcodes_re, postcodes):\n",
    "        new_zip = re.search(postcodes_re, postcodes).group(1)\n",
    "        output.append(new_zip)\n",
    "\n",
    "    return ', '.join(str(x) for x in output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrect and Inconsistent City Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once again, this is similar in function to audit_street\n",
    "def audit_city(city_dict, city_ex):\n",
    "    m = cities_re.search(city_ex)\n",
    "    if m:\n",
    "        city_group = m.group()\n",
    "        city_dict[city_group].add(city_ex)\n",
    "\n",
    "#Same function as is_postcode, but for addr:city\n",
    "def is_city(elem):\n",
    "    return (elem.attrib['k'] == \"addr:city\")\n",
    "\n",
    "#Same function as audit_s, but for city values.\n",
    "def audit_C(osmfile):\n",
    "    city_dict = defaultdict(set)\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "\n",
    "    # get an iterable\n",
    "    iterable = ET.iterparse(osm_file, events=(\"start\", \"end\"))\n",
    "\n",
    "    # turn it into an iterator\n",
    "    iterable = iter(iterable)\n",
    "\n",
    "    # get the root element\n",
    "    event, root = iterable.next()\n",
    "\n",
    "    for event, elem in iterable:\n",
    "        if event == \"end\" and (elem.tag == \"node\" or elem.tag == \"way\"):\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_city(tag):\n",
    "                    audit_city(city_dict, tag.attrib['v'])\n",
    "        root.clear()\n",
    "    osm_file.close()\n",
    "    return city_dict\n",
    "\n",
    "\"\"\" Same function as the update_street, except instead of it skipping the\n",
    "the matched tuple, instead it instead uses the ofallon_mapping dict to correct the\n",
    "inconsistency of some cities being listed as O'fallon and some as O fallon. \n",
    "\"\"\"\n",
    "def update_city(name):\n",
    "    OFALLON = ('o')\n",
    "    ofallon_mapping = {\"O\": \"O'\"}\n",
    "    city_mapping = {\"St\": \"Saint\",\n",
    "                \"St.\": \"Saint\",\n",
    "                \"bridgeton\" : \"Bridgeton\",\n",
    "                \"drive-through\": \"O'Fallon\",\n",
    "                \"Bass\": \"Saint\",\n",
    "                \"Pro\": \"Charles\",\n",
    "                \"Drive\": \"\",\n",
    "                \"UNINCORPORATED\": \"Saint Peters\",\n",
    "                }\n",
    "\n",
    "    if name.lower().startswith(OFALLON):\n",
    "        return ''.join((ofallon_mapping.get(part, part)).title() for part in name.split())\n",
    "    return ' '.join((city_mapping.get(part, part)).title() for part in name.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for database - SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to prepare the data to be inserted into a SQL database using OSM_to_CSV.py. To do so I parsed the elements in the OSM XML file, checking for any problem characters, before breaking each tag into seperate parts, then putting each of these parts into a seperate dictionary. These dictionaries were then used to create individual csv files with each csv file correlating to a seperate table in the soon to be established database. These csv files can then easily be imported to a SQL database as tables. The “shape_element()” function is used to transform each element in the correct format, the process_map() function then is used to pull the \"shaped elements\" from the dictionaries created earlier and write them to their own csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SQLite Database and Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made use of python sqlite3 library to create a flatfile SQLite database. I created nodes, nodes_tags, ways, ways_tags, ways_nodes tables and parsed the csv files to fill the respective tables. Also of note I included DROP TABLE IF EXISTS before each table was created. This was to make updating the database much simpler whenever testing code, as I often included bogus fields or rows to test or edited the file itself and time or processing constraints werent a worry so it was much easier then performing an UPDATE statement or other methods that would be more commonly used in real world scenarios. This removed a major concern, which was corruption, duplication, or otherwise major integrity issues with the database itself. I also included a CREATE TABLE IF NOT EXISTS statement, which would double of the safety of the previous measure by causing the database to not create a table if the previous DROP TABLE IF EXISTS didn't run properly. \n",
    "\n",
    "Below is a sample of the code found in the creating_db.py file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "db = 'osm_stchas.sqlite'\n",
    "\n",
    "# Connecting to the database\n",
    "con = sqlite3.connect(db)\n",
    "con.text_factory = str\n",
    "cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we drop the nodes_tags table if it exists to save us from data integrity issues when rerunning this file.\n",
    "cursor.execute('''\n",
    "    DROP TABLE IF EXISTS nodes_tags\n",
    "''')\n",
    "con.commit()\n",
    "\n",
    "# Here we create nodes_tags table.\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS nodes_tags(id INTEGER, key TEXT, value TEXT, type TEXT)\n",
    "''')\n",
    "con.commit()\n",
    "\n",
    "with open('nodes_tags.csv', 'rb') as f:\n",
    "    dr = csv.DictReader(f)\n",
    "    iterate_db = [(i[b'id'], i[b'key'], i[b'value'], i[b'type']) for i in dr]\n",
    "\n",
    "# Lets go ahead and insert the data into the nodes_tags tables from the 'nodes_tags.csv' file.\n",
    "cursor.executemany('INSERT INTO nodes_tags(id, key, value, type) VALUES(?, ?, ?, ?);', iterate_db)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we can actually explore the data now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File size:\n",
    "\n",
    "osm_stchas.sqlite --- 125.6MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Nodes\n",
    "def number_of_nodes():\n",
    "    output = cursor.execute('SELECT COUNT(*) FROM nodes')\n",
    "    return output.fetchone()[0]\n",
    "print('Number of nodes: %d' % (number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of nodes: 723413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Ways\n",
    "def number_of_ways():\n",
    "    output = cursor.execute('SELECT COUNT(*) FROM ways')\n",
    "    return output.fetchone()[0]\n",
    "print('Number of ways: %d' %(number_of_ways()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of ways: 70274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Unique Users\n",
    "def number_of_unique_users():\n",
    "    output = cursor.execute('SELECT COUNT(DISTINCT e.uid) FROM \\\n",
    "                         (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e')\n",
    "    return output.fetchone()[0]\n",
    "print('Number of unique users: %d' %(number_of_unique_users()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of unique users: 846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Contributing Users\n",
    "def most_contributing_users():\n",
    "    output = cursor.execute('SELECT e.user, COUNT(*) as num FROM \\\n",
    "                         (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "                         GROUP BY e.user \\\n",
    "                         ORDER BY num DESC \\\n",
    "                         LIMIT 10 ')\n",
    "    pprint(output.fetchall())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(u'CmdrThor', 340815),\n",
    " (u'fearsyth', 121261),\n",
    " (u'EbersGIS', 18531),\n",
    " (u'seyahmit', 17302),\n",
    " (u'Dahc', 16100),\n",
    " (u'localfixerupper', 14979),\n",
    " (u'sannkc', 13802),\n",
    " (u'Paul Wiltsey', 13167),\n",
    " (u'dustinturp', 11516),\n",
    " (u'Nips Whipple', 10002)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Users Who Contributed Once\n",
    "def number_of_users_contributed_once():\n",
    "    output = cursor.execute('SELECT COUNT(*) FROM \\\n",
    "                             (SELECT e.user, COUNT(*) as num FROM \\\n",
    "                                 (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "                                  GROUP BY e.user \\\n",
    "                                  HAVING num = 1) u')\n",
    "    return output.fetchone()[0]\n",
    "print('Number of users who have contributed once: %d' % number_of_users_contributed_once())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of users who have contributed once: 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort cities by count\n",
    "\n",
    "def cities_by_count():\n",
    "    output = cursor.execute(\"SELECT tags.value, COUNT(*) as count FROM \\\n",
    "                                (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) tags \\\n",
    "                                    WHERE tags.key LIKE 'city' \\\n",
    "                                    GROUP BY tags.value \\\n",
    "                                    ORDER BY count DESC;\")\n",
    "    pprint(output.fetchall())\n",
    "    return None\n",
    "\n",
    "cities_by_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(u'Saint Peters', 23913),\n",
    " (u\"O'Fallon\", 19601),\n",
    " (u'unincorporated', 16603),\n",
    " (u'Florissant', 16407),\n",
    " (u'Maryland Heights', 12125),\n",
    " (u'Bridgeton', 6797),\n",
    " (u'Saint Charles', 4609),\n",
    " (u'Weldon Spring', 2291),\n",
    " (u'Cottleville', 2005),\n",
    " (u'Hazelwood', 1792),\n",
    " (u'Dardenne Prairie', 1314),\n",
    " (u'Champ', 82),\n",
    " (u'Saint Paul', 46),\n",
    " (u'Weldon Spring Heights', 36),\n",
    " (u'Saint Ann', 18),\n",
    " (u'Saint Louis', 11),\n",
    " (u'Berkeley', 10),\n",
    " (u'Woodson Terrace', 2),\n",
    " (u'Overland', 2),\n",
    " (u'Ferguson', 2),\n",
    " (u'Calverton Park', 2),\n",
    " (u'Saint John', 1),\n",
    " (u'Saint Charles ', 1),\n",
    " (u'Golden Eagle', 1),\n",
    " (u'Edmundson', 1),\n",
    " (u'Charlack', 1),\n",
    " (u'Carsonville', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postcodes_by_count():\n",
    "    output = cursor.execute(\"SELECT tags.value, COUNT(*) as count FROM \\\n",
    "                            (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) tags \\\n",
    "                                WHERE tags.key='postcode' \\\n",
    "                                GROUP BY tags.value \\\n",
    "                                ORDER BY count DESC;\")\n",
    "    pprint(output.fetchall())\n",
    "    return None\n",
    "\n",
    "postcodes_by_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(u'63376', 29809),\n",
    " (u'63031', 20884),\n",
    " (u'63304', 16139),\n",
    " (u'63043', 12162),\n",
    " (u'63366', 11537),\n",
    " (u'63303', 9335),\n",
    " (u'63368', 7374),\n",
    " (u'63044', 6877),\n",
    " (u'63033', 2257),\n",
    " (u'63301', 2218),\n",
    " (u'63034', 1247),\n",
    " (u'63045', 782),\n",
    " (u'63042', 51),\n",
    " (u'63146', 16),\n",
    " (u'63074', 11),\n",
    " (u'63114', 10),\n",
    " (u'63134', 6),\n",
    " (u'63145', 3),\n",
    " (u'63017', 3),\n",
    " (u'63135', 2),\n",
    " (u'63338', 1),\n",
    " (u'63201', 1),\n",
    " (u'63133', 1),\n",
    " (u'62036', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for Top 10 Amenities in St Charles\n",
    "query = \"SELECT value, COUNT(*) as num FROM nodes_tags \\\n",
    "            WHERE key='amenity' \\\n",
    "            GROUP BY value \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 20\"\n",
    "\n",
    "# Top 10 Amenities in St Charles\n",
    "def top_ten_amenities_in_st_charles():\n",
    "    output = cursor.execute(query)\n",
    "    pprint(output.fetchall())\n",
    "    return None\n",
    "\n",
    "print('Top 10 Amenities:\\n')\n",
    "top_ten_amenities_in_st_charles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top 10 Amenities:\n",
    "\n",
    "[(u'fast_food', 68),\n",
    " (u'place_of_worship', 66),\n",
    " (u'restaurant', 62),\n",
    " (u'school', 61),\n",
    " (u'bench', 44),\n",
    " (u'parking', 36),\n",
    " (u'toilets', 26),\n",
    " (u'fountain', 26),\n",
    " (u'bank', 21),\n",
    " (u'drinking_water', 19),\n",
    " (u'pharmacy', 18),\n",
    " (u'waste_basket', 16),\n",
    " (u'shelter', 13),\n",
    " (u'vending_machine', 12),\n",
    " (u'cafe', 12),\n",
    " (u'atm', 11),\n",
    " (u'grave_yard', 10),\n",
    " (u'fuel', 10),\n",
    " (u'fire_station', 9),\n",
    " (u'post_office', 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Cuisines in St Charles\n",
    "query = \"SELECT value, COUNT(*) as num FROM ways_tags \\\n",
    "            WHERE key='cuisine' \\\n",
    "            GROUP BY value \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 10\"\n",
    "\n",
    "def cuisines_in_st_charles():\n",
    "    output = cursor.execute(query)\n",
    "    pprint(output.fetchall())\n",
    "    return None\n",
    "\n",
    "print('Top 10 Cuisines in St Charles:\\n')\n",
    "cuisines_in_st_charles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top 10 Cuisines in St Charles:\n",
    "\n",
    "[(u'burger', 41),\n",
    " (u'american', 13),\n",
    " (u'sandwich', 11),\n",
    " (u'chicken', 7),\n",
    " (u'tex-mex', 6),\n",
    " (u'steak_house', 4),\n",
    " (u'pizza', 4),\n",
    " (u'italian', 4),\n",
    " (u'seafood', 3),\n",
    " (u'mexican', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Types of Shops\n",
    "query = \"SELECT value, COUNT(*) as num FROM nodes_tags \\\n",
    "            WHERE key='shop' \\\n",
    "            GROUP BY value \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 10\"\n",
    "\n",
    "def shops_in_st_charles():\n",
    "    output = cursor.execute(query)\n",
    "    pprint(output.fetchall())\n",
    "    return None\n",
    "\n",
    "print('Different types of shops:\\n')\n",
    "shops_in_st_charles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Different types of shops:\n",
    "\n",
    "[(u'clothes', 36),\n",
    " (u'hairdresser', 19),\n",
    " (u'convenience', 16),\n",
    " (u'beauty', 13),\n",
    " (u'supermarket', 12),\n",
    " (u'jewelry', 7),\n",
    " (u'variety_store', 6),\n",
    " (u'optician', 6),\n",
    " (u'mobile_phone', 6),\n",
    " (u'gift', 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popular Cafes in St Charles\n",
    "def most_popular_cafes():\n",
    "    output = cursor.execute('SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "                          FROM nodes_tags \\\n",
    "                            JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"coffee_shop\") AS cafes \\\n",
    "                            ON nodes_tags.id = cafes.id \\\n",
    "                            WHERE nodes_tags.key=\"name\"\\\n",
    "                            GROUP BY nodes_tags.value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 10' ) # Remove this limit to see the complete list of cafes\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print('Most popular cafes in St Charles: \\n')\n",
    "most_popular_cafes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Most popular cafes in St Charles:\n",
    "    \n",
    "[(u'Starbucks', 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas for Improvement and Anticipated Problems :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above review and analysis, there are a few stand out features as a resident of this area myself. First, why is starbucks considered the only coffee shop? So categories of shops and other details as such definitely need to be looked into in the future.\n",
    "Another thing that can be improved, is creating a list or dictionary like we did for the mapping of common brand name or often misspelled bussinesses to ensure that there is a standardization of naming.\n",
    "However, lastly, the most important and certainly most difficult aspect would be keeping this dataset up to date. Since the dataset is not a cached or saved dataset and businesses and roads continue to change and update it would be an enormous responsibility to keep the data clean AND real time. This is a very eye opening project to size and scale of the jobs that we generalize as \"big data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the python help, I took advantage of four sources:\n",
    "* https://stackoverflow.com\n",
    "* https://docs.python.org/\n",
    "* Multiple users and moderators of the official Python Discord channel. With specific debuging and optimization help (cutting off an insane 700 seconds of runtime) from users salt-die and Dan6erbond.\n",
    "* Alumni from this program and WGU, including Michael Kuehn and Pranav Suri and Sriram Jaju, via their personal githubs, and linked in messages \n",
    "\n",
    "    * https://sriramjaju.github.io/2017-06-16-openstreetmap-data-wrangling-with-sql/\n",
    "    * https://github.com/pranavsuri/Data-Analyst-Nanodegree\n",
    "    * https://github.com/mkuehn10/P3-Wrangle-OpenStreetMap-Data\n",
    "\n",
    "For sql I didn't need much help, but often used https://www.w3schools.com/sql/default.asp for reminders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
